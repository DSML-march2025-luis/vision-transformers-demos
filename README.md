
# Vision Transformers (ViTs)

Vision Transformers (ViTs) are a type of deep learning model for image recognition that use the transformer architecture (originally developed for NLP) instead of convolutional neural networks (CNNs).

Key ideas:
- An image is split into patches (e.g., 16x16 pixels).
- Each patch is flattened and treated like a token (similar to words in NLP).
- These tokens are processed using a standard transformer encoder.
- The model learns global relationships between patches, not just local features (like CNNs do).

<br>

## How it works

- Code ➜
- Colab ➜

<br>

## Image Classification


- Code ➜
- Colab ➜

<br>

## Object Detection

- Code ➜ [object_detection/object_detection_demo.ipynb](object_detection/object_detection_demo.ipynb)
- Colab ➜

