
# Vision Transformers (ViTs)

Vision Transformers (ViTs) are a type of deep learning model for image recognition that use the transformer architecture (originally developed for NLP) instead of convolutional neural networks (CNNs).

Key ideas:
- An image is split into patches (e.g., 16x16 pixels).
- Each patch is flattened and treated like a token (similar to words in NLP).
- These tokens are processed using a standard transformer encoder.
- The model learns global relationships between patches, not just local features (like CNNs do).

<br>


## How it works

- Code ➜

- Colab ➜

<br>


## Image Classification

- Code ➜ image_classification/ViT_Image_Classification_CodeAlong.ipynb

- Colab ➜  https://colab.research.google.com/drive/1lY-8HjXVrAoegGcinO-QJyxYfRfXJALc#scrollTo=1HYlwi9gDdS1

<br>


## Object Detection

- Initial template for codealong ➜ [https://colab.research.google.com/drive/13RG-DLU7RT-uHvP0Zw2EAh8jxfqlzgii](https://colab.research.google.com/drive/13RG-DLU7RT-uHvP0Zw2EAh8jxfqlzgii)

- Final Code (GitHub) ➜ [object_detection/object_detection_demo.ipynb](object_detection/object_detection_demo.ipynb)

- Final Code (Colab) ➜ [https://colab.research.google.com/drive/1tC1-xvT87-naDI8Mb_kNtdm7J1deiRJO](https://colab.research.google.com/drive/1tC1-xvT87-naDI8Mb_kNtdm7J1deiRJO)


